{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "135825fb-a26d-4a54-bda2-dc7fcd3dae39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найдено дат с погодой: 296\n",
      "Первые 5 дат: ['2025-01-28', '2025-01-29', '2025-01-30', '2025-01-31', '2025-02-01']\n",
      "Обрабатываем 2025-01-28 ...\n",
      "  ⚠ Нет данных (после десериализации) для 2025-01-28\n",
      "Обрабатываем 2025-01-29 ...\n",
      "  ⚠ Нет данных (после десериализации) для 2025-01-29\n",
      "Обрабатываем 2025-01-30 ...\n",
      "Обрабатываем 2025-01-31 ...\n",
      "Обрабатываем 2025-02-01 ...\n",
      "Обрабатываем 2025-02-02 ...\n",
      "Обрабатываем 2025-02-03 ...\n",
      "Обрабатываем 2025-02-04 ...\n",
      "Обрабатываем 2025-02-05 ...\n",
      "Обрабатываем 2025-02-06 ...\n",
      "Обрабатываем 2025-02-07 ...\n",
      "Обрабатываем 2025-02-08 ...\n",
      "Обрабатываем 2025-02-09 ...\n",
      "Обрабатываем 2025-02-10 ...\n",
      "Обрабатываем 2025-02-11 ...\n",
      "Обрабатываем 2025-02-12 ...\n",
      "Обрабатываем 2025-02-13 ...\n",
      "Обрабатываем 2025-02-14 ...\n",
      "Обрабатываем 2025-02-15 ...\n",
      "Обрабатываем 2025-02-16 ...\n",
      "Обрабатываем 2025-02-17 ...\n",
      "Обрабатываем 2025-02-18 ...\n",
      "Обрабатываем 2025-02-19 ...\n",
      "Обрабатываем 2025-02-20 ...\n",
      "Обрабатываем 2025-02-21 ...\n",
      "Обрабатываем 2025-02-22 ...\n",
      "Обрабатываем 2025-02-23 ...\n",
      "Обрабатываем 2025-02-24 ...\n",
      "Обрабатываем 2025-02-25 ...\n",
      "Обрабатываем 2025-02-26 ...\n",
      "Обрабатываем 2025-02-27 ...\n",
      "  ⚠ Нет данных (после десериализации) для 2025-02-27\n",
      "Обрабатываем 2025-02-28 ...\n",
      "Обрабатываем 2025-03-01 ...\n",
      "Обрабатываем 2025-03-02 ...\n",
      "Обрабатываем 2025-03-03 ...\n",
      "Обрабатываем 2025-03-04 ...\n",
      "Обрабатываем 2025-03-05 ...\n",
      "Обрабатываем 2025-03-06 ...\n",
      "Обрабатываем 2025-03-07 ...\n",
      "Обрабатываем 2025-03-08 ...\n",
      "Обрабатываем 2025-03-09 ...\n",
      "Обрабатываем 2025-03-10 ...\n",
      "Обрабатываем 2025-03-11 ...\n",
      "Обрабатываем 2025-03-12 ...\n",
      "Обрабатываем 2025-03-13 ...\n",
      "Обрабатываем 2025-03-14 ...\n",
      "Обрабатываем 2025-03-15 ...\n",
      "Обрабатываем 2025-03-16 ...\n",
      "Обрабатываем 2025-03-17 ...\n",
      "Обрабатываем 2025-03-18 ...\n",
      "Обрабатываем 2025-03-19 ...\n",
      "Обрабатываем 2025-03-20 ...\n",
      "Обрабатываем 2025-03-21 ...\n",
      "Обрабатываем 2025-03-22 ...\n",
      "Обрабатываем 2025-03-23 ...\n",
      "Обрабатываем 2025-03-24 ...\n",
      "Обрабатываем 2025-03-25 ...\n",
      "Обрабатываем 2025-03-26 ...\n",
      "Обрабатываем 2025-03-27 ...\n",
      "Обрабатываем 2025-03-28 ...\n",
      "Обрабатываем 2025-03-29 ...\n",
      "Обрабатываем 2025-03-30 ...\n",
      "Обрабатываем 2025-03-31 ...\n",
      "Обрабатываем 2025-04-01 ...\n",
      "Обрабатываем 2025-04-02 ...\n",
      "Обрабатываем 2025-04-03 ...\n",
      "Обрабатываем 2025-04-04 ...\n",
      "Обрабатываем 2025-04-05 ...\n",
      "Обрабатываем 2025-04-06 ...\n",
      "Обрабатываем 2025-04-07 ...\n",
      "Обрабатываем 2025-04-08 ...\n",
      "Обрабатываем 2025-04-09 ...\n",
      "Обрабатываем 2025-04-10 ...\n",
      "Обрабатываем 2025-04-11 ...\n",
      "Обрабатываем 2025-04-12 ...\n",
      "Обрабатываем 2025-04-13 ...\n",
      "Обрабатываем 2025-04-14 ...\n",
      "Обрабатываем 2025-04-15 ...\n",
      "Обрабатываем 2025-04-16 ...\n",
      "Обрабатываем 2025-04-17 ...\n",
      "Обрабатываем 2025-04-18 ...\n",
      "Обрабатываем 2025-04-19 ...\n",
      "Обрабатываем 2025-04-20 ...\n",
      "Обрабатываем 2025-04-21 ...\n",
      "Обрабатываем 2025-04-22 ...\n",
      "Обрабатываем 2025-04-23 ...\n",
      "Обрабатываем 2025-04-24 ...\n",
      "Обрабатываем 2025-04-25 ...\n",
      "Обрабатываем 2025-04-26 ...\n",
      "Обрабатываем 2025-04-27 ...\n",
      "Обрабатываем 2025-04-28 ...\n",
      "Обрабатываем 2025-04-29 ...\n",
      "Обрабатываем 2025-04-30 ...\n",
      "Обрабатываем 2025-05-01 ...\n",
      "Обрабатываем 2025-05-02 ...\n",
      "Обрабатываем 2025-05-03 ...\n",
      "Обрабатываем 2025-05-04 ...\n",
      "Обрабатываем 2025-05-05 ...\n",
      "Обрабатываем 2025-05-06 ...\n",
      "Обрабатываем 2025-05-07 ...\n",
      "Обрабатываем 2025-05-08 ...\n",
      "Обрабатываем 2025-05-09 ...\n",
      "Обрабатываем 2025-05-10 ...\n",
      "  ⚠ Нет данных (после десериализации) для 2025-05-10\n",
      "Обрабатываем 2025-05-11 ...\n",
      "Обрабатываем 2025-05-12 ...\n",
      "Обрабатываем 2025-05-13 ...\n",
      "Обрабатываем 2025-05-14 ...\n",
      "Обрабатываем 2025-05-15 ...\n",
      "Обрабатываем 2025-05-16 ...\n",
      "Обрабатываем 2025-05-17 ...\n",
      "Обрабатываем 2025-05-18 ...\n",
      "Обрабатываем 2025-05-19 ...\n",
      "Обрабатываем 2025-05-20 ...\n",
      "Обрабатываем 2025-05-21 ...\n",
      "Обрабатываем 2025-05-22 ...\n",
      "Обрабатываем 2025-05-23 ...\n",
      "Обрабатываем 2025-05-24 ...\n",
      "Обрабатываем 2025-05-25 ...\n",
      "Обрабатываем 2025-05-26 ...\n",
      "Обрабатываем 2025-05-27 ...\n",
      "Обрабатываем 2025-05-28 ...\n",
      "Обрабатываем 2025-05-29 ...\n",
      "Обрабатываем 2025-05-30 ...\n",
      "Обрабатываем 2025-05-31 ...\n",
      "Обрабатываем 2025-06-01 ...\n",
      "Обрабатываем 2025-06-02 ...\n",
      "Обрабатываем 2025-06-03 ...\n",
      "Обрабатываем 2025-06-04 ...\n",
      "Обрабатываем 2025-06-05 ...\n",
      "Обрабатываем 2025-06-06 ...\n",
      "Обрабатываем 2025-06-07 ...\n",
      "Обрабатываем 2025-06-08 ...\n",
      "Обрабатываем 2025-06-09 ...\n",
      "Обрабатываем 2025-06-10 ...\n",
      "Обрабатываем 2025-06-11 ...\n",
      "Обрабатываем 2025-06-12 ...\n",
      "Обрабатываем 2025-06-13 ...\n",
      "Обрабатываем 2025-06-14 ...\n",
      "Обрабатываем 2025-06-15 ...\n",
      "Обрабатываем 2025-06-16 ...\n",
      "Обрабатываем 2025-06-17 ...\n",
      "Обрабатываем 2025-06-18 ...\n",
      "Обрабатываем 2025-06-19 ...\n",
      "Обрабатываем 2025-06-20 ...\n",
      "Обрабатываем 2025-06-21 ...\n",
      "Обрабатываем 2025-06-22 ...\n",
      "Обрабатываем 2025-06-23 ...\n",
      "Обрабатываем 2025-06-24 ...\n",
      "Обрабатываем 2025-06-25 ...\n",
      "Обрабатываем 2025-06-26 ...\n",
      "Обрабатываем 2025-06-27 ...\n",
      "Обрабатываем 2025-06-28 ...\n",
      "Обрабатываем 2025-06-29 ...\n",
      "Обрабатываем 2025-06-30 ...\n",
      "Обрабатываем 2025-07-01 ...\n",
      "Обрабатываем 2025-07-02 ...\n",
      "Обрабатываем 2025-07-03 ...\n",
      "Обрабатываем 2025-07-04 ...\n",
      "Обрабатываем 2025-07-05 ...\n",
      "Обрабатываем 2025-07-06 ...\n",
      "Обрабатываем 2025-07-07 ...\n",
      "Обрабатываем 2025-07-08 ...\n",
      "Обрабатываем 2025-07-09 ...\n",
      "Обрабатываем 2025-07-10 ...\n",
      "Обрабатываем 2025-07-11 ...\n",
      "Обрабатываем 2025-07-12 ...\n",
      "Обрабатываем 2025-07-13 ...\n",
      "Обрабатываем 2025-07-14 ...\n",
      "Обрабатываем 2025-07-15 ...\n",
      "Обрабатываем 2025-07-16 ...\n",
      "Обрабатываем 2025-07-17 ...\n",
      "Обрабатываем 2025-07-18 ...\n",
      "Обрабатываем 2025-07-19 ...\n",
      "Обрабатываем 2025-07-20 ...\n",
      "Обрабатываем 2025-07-21 ...\n",
      "Обрабатываем 2025-07-22 ...\n",
      "Обрабатываем 2025-07-23 ...\n",
      "Обрабатываем 2025-07-24 ...\n",
      "Обрабатываем 2025-07-25 ...\n",
      "Обрабатываем 2025-07-26 ...\n",
      "Обрабатываем 2025-07-27 ...\n",
      "Обрабатываем 2025-07-28 ...\n",
      "Обрабатываем 2025-07-29 ...\n",
      "Обрабатываем 2025-07-30 ...\n",
      "Обрабатываем 2025-07-31 ...\n",
      "Обрабатываем 2025-08-01 ...\n",
      "Обрабатываем 2025-08-02 ...\n",
      "Обрабатываем 2025-08-03 ...\n",
      "Обрабатываем 2025-08-04 ...\n",
      "Обрабатываем 2025-08-05 ...\n",
      "Обрабатываем 2025-08-06 ...\n",
      "Обрабатываем 2025-08-07 ...\n",
      "Обрабатываем 2025-08-08 ...\n",
      "Обрабатываем 2025-08-09 ...\n",
      "Обрабатываем 2025-08-10 ...\n",
      "Обрабатываем 2025-08-11 ...\n",
      "Обрабатываем 2025-08-12 ...\n",
      "Обрабатываем 2025-08-13 ...\n",
      "Обрабатываем 2025-08-14 ...\n",
      "Обрабатываем 2025-08-15 ...\n",
      "Обрабатываем 2025-08-16 ...\n",
      "Обрабатываем 2025-08-17 ...\n",
      "Обрабатываем 2025-08-18 ...\n",
      "Обрабатываем 2025-08-19 ...\n",
      "Обрабатываем 2025-08-20 ...\n",
      "Обрабатываем 2025-08-21 ...\n",
      "Обрабатываем 2025-08-22 ...\n",
      "Обрабатываем 2025-08-23 ...\n",
      "Обрабатываем 2025-08-24 ...\n",
      "Обрабатываем 2025-08-25 ...\n",
      "Обрабатываем 2025-08-26 ...\n",
      "Обрабатываем 2025-08-27 ...\n",
      "Обрабатываем 2025-08-28 ...\n",
      "Обрабатываем 2025-08-29 ...\n",
      "Обрабатываем 2025-08-30 ...\n",
      "Обрабатываем 2025-08-31 ...\n",
      "Обрабатываем 2025-09-01 ...\n",
      "Обрабатываем 2025-09-02 ...\n",
      "Обрабатываем 2025-09-03 ...\n",
      "Обрабатываем 2025-09-04 ...\n",
      "Обрабатываем 2025-09-05 ...\n",
      "Обрабатываем 2025-09-06 ...\n",
      "Обрабатываем 2025-09-07 ...\n",
      "Обрабатываем 2025-09-08 ...\n",
      "Обрабатываем 2025-09-09 ...\n",
      "Обрабатываем 2025-09-10 ...\n",
      "Обрабатываем 2025-09-11 ...\n",
      "Обрабатываем 2025-09-12 ...\n",
      "Обрабатываем 2025-09-13 ...\n",
      "Обрабатываем 2025-09-14 ...\n",
      "Обрабатываем 2025-09-15 ...\n",
      "Обрабатываем 2025-09-16 ...\n",
      "Обрабатываем 2025-09-17 ...\n",
      "Обрабатываем 2025-09-18 ...\n",
      "Обрабатываем 2025-09-19 ...\n",
      "Обрабатываем 2025-09-20 ...\n",
      "Обрабатываем 2025-09-21 ...\n",
      "Обрабатываем 2025-09-22 ...\n",
      "Обрабатываем 2025-09-23 ...\n",
      "Обрабатываем 2025-09-24 ...\n",
      "Обрабатываем 2025-09-25 ...\n",
      "Обрабатываем 2025-09-26 ...\n",
      "Обрабатываем 2025-09-27 ...\n",
      "Обрабатываем 2025-09-28 ...\n",
      "Обрабатываем 2025-09-29 ...\n",
      "Обрабатываем 2025-09-30 ...\n",
      "Обрабатываем 2025-10-01 ...\n",
      "Обрабатываем 2025-10-02 ...\n",
      "Обрабатываем 2025-10-03 ...\n",
      "Обрабатываем 2025-10-04 ...\n",
      "Обрабатываем 2025-10-05 ...\n",
      "Обрабатываем 2025-10-06 ...\n",
      "Обрабатываем 2025-10-07 ...\n",
      "  ⚠ Нет данных (после десериализации) для 2025-10-07\n",
      "Обрабатываем 2025-10-08 ...\n",
      "Обрабатываем 2025-10-09 ...\n",
      "Обрабатываем 2025-10-10 ...\n",
      "Обрабатываем 2025-10-11 ...\n",
      "Обрабатываем 2025-10-12 ...\n",
      "Обрабатываем 2025-10-13 ...\n",
      "Обрабатываем 2025-10-14 ...\n",
      "Обрабатываем 2025-10-15 ...\n",
      "Обрабатываем 2025-10-16 ...\n",
      "Обрабатываем 2025-10-17 ...\n",
      "Обрабатываем 2025-10-18 ...\n",
      "Обрабатываем 2025-10-19 ...\n",
      "Обрабатываем 2025-10-20 ...\n",
      "Обрабатываем 2025-10-21 ...\n",
      "Обрабатываем 2025-10-22 ...\n",
      "Обрабатываем 2025-10-23 ...\n",
      "Обрабатываем 2025-10-24 ...\n",
      "Обрабатываем 2025-10-25 ...\n",
      "Обрабатываем 2025-10-26 ...\n",
      "Обрабатываем 2025-10-27 ...\n",
      "Обрабатываем 2025-10-28 ...\n",
      "Обрабатываем 2025-10-29 ...\n",
      "Обрабатываем 2025-10-30 ...\n",
      "Обрабатываем 2025-10-31 ...\n",
      "Обрабатываем 2025-11-01 ...\n",
      "Обрабатываем 2025-11-02 ...\n",
      "Обрабатываем 2025-11-03 ...\n",
      "Обрабатываем 2025-11-04 ...\n",
      "Обрабатываем 2025-11-05 ...\n",
      "Обрабатываем 2025-11-06 ...\n",
      "Обрабатываем 2025-11-07 ...\n",
      "Обрабатываем 2025-11-08 ...\n",
      "Обрабатываем 2025-11-09 ...\n",
      "Обрабатываем 2025-11-10 ...\n",
      "Обрабатываем 2025-11-11 ...\n",
      "Обрабатываем 2025-11-12 ...\n",
      "Обрабатываем 2025-11-13 ...\n",
      "Обрабатываем 2025-11-14 ...\n",
      "Обрабатываем 2025-11-15 ...\n",
      "Обрабатываем 2025-11-16 ...\n",
      "Обрабатываем 2025-11-17 ...\n",
      "Обрабатываем 2025-11-18 ...\n",
      "Обрабатываем 2025-11-19 ...\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 209\u001b[39m\n\u001b[32m    206\u001b[39m out_pq  = os.path.join(OUTPUT_DIR, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mweather_uid_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_object_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_full.parquet\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    208\u001b[39m weather_full.to_csv(out_csv, index=\u001b[38;5;28;01mFalse\u001b[39;00m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m \u001b[43mweather_full\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_pq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Готово.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    212\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCSV:\u001b[39m\u001b[33m\"\u001b[39m, out_csv)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:3124\u001b[39m, in \u001b[36mDataFrame.to_parquet\u001b[39m\u001b[34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[39m\n\u001b[32m   3043\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3044\u001b[39m \u001b[33;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[32m   3045\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3120\u001b[39m \u001b[33;03m>>> content = f.read()\u001b[39;00m\n\u001b[32m   3121\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3122\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[32m-> \u001b[39m\u001b[32m3124\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3125\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3132\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3133\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py:478\u001b[39m, in \u001b[36mto_parquet\u001b[39m\u001b[34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(partition_cols, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    477\u001b[39m     partition_cols = [partition_cols]\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m impl = \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m path_or_buf: FilePath | WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] = io.BytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[32m    482\u001b[39m impl.write(\n\u001b[32m    483\u001b[39m     df,\n\u001b[32m    484\u001b[39m     path_or_buf,\n\u001b[32m   (...)\u001b[39m\u001b[32m    490\u001b[39m     **kwargs,\n\u001b[32m    491\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py:68\u001b[39m, in \u001b[36mget_engine\u001b[39m\u001b[34m(engine)\u001b[39m\n\u001b[32m     65\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m     66\u001b[39m             error_msgs += \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m - \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(err)\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     69\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to find a usable engine; \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     70\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtried using: \u001b[39m\u001b[33m'\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mfastparquet\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     71\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mA suitable version of \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     72\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpyarrow or fastparquet is required for parquet \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     73\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msupport.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     74\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTrying to import the above resulted in these errors:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     75\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msgs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     76\u001b[39m     )\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m PyArrowImpl()\n",
      "\u001b[31mImportError\u001b[39m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "import javaobj  # pip install javaobj-py3\n",
    "\n",
    "# ---------------- НАСТРОЙКИ ----------------\n",
    "\n",
    "DB_URI = \"postgresql://postgres:smartgrid@172.31.168.2/solar_db\"\n",
    "user_object_id = 70\n",
    "OUTPUT_DIR = \"data\"\n",
    "\n",
    "# -------------------------------------------\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Создаём engine для подключения к PostgreSQL\n",
    "engine = create_engine(DB_URI)\n",
    "\n",
    "# ========== ВСПОМОГАТЕЛЬНЫЕ ФУНКЦИИ ==========\n",
    "\n",
    "def deserialize_java_object(binary_value):\n",
    "    \"\"\"Десериализация бинарного поля (bytea) в Java-объект через javaobj.loads.\"\"\"\n",
    "    if binary_value is None:\n",
    "        return None\n",
    "    if isinstance(binary_value, memoryview):\n",
    "        binary_value = binary_value.tobytes()\n",
    "    try:\n",
    "        return javaobj.loads(binary_value)\n",
    "    except Exception as e:\n",
    "        logger.error(\"Грешка при десериализация на Java обект: %s\", e)\n",
    "        return None\n",
    "\n",
    "\n",
    "def unwrap_value(obj):\n",
    "    \"\"\"Если obj имеет атрибут .value (JavaDouble/JavaInteger/JavaString) — вернуть obj.value.\"\"\"\n",
    "    return getattr(obj, \"value\", obj)\n",
    "\n",
    "\n",
    "def extract_forecast_data(forecast_obj):\n",
    "    \"\"\"\n",
    "    Извлекает из forecast_obj список записей:\n",
    "      - time   (строка \"YYYY-MM-DD HH:MM\")\n",
    "      - temp_c (float)\n",
    "      - cloud  (int)\n",
    "    \"\"\"\n",
    "    days = getattr(forecast_obj, \"forecastday\", None)\n",
    "    if days is None:\n",
    "        return []\n",
    "    try:\n",
    "        days = list(days)\n",
    "    except Exception as e:\n",
    "        logger.error(\"Неуспешно преобразуване на forecastday в списък: %s\", e)\n",
    "        return []\n",
    "\n",
    "    data = []\n",
    "    for day in days:\n",
    "        hours = getattr(day, \"hour\", None)\n",
    "        if hours is None:\n",
    "            continue\n",
    "        try:\n",
    "            hours = list(hours)\n",
    "        except Exception as e:\n",
    "            logger.error(\"Неуспешно преобразуване на hour в списък: %s\", e)\n",
    "            continue\n",
    "\n",
    "        for hour in hours:\n",
    "            t_raw     = getattr(hour, \"time\", None)\n",
    "            temp_raw  = getattr(hour, \"tempC\", None)\n",
    "            cloud_raw = getattr(hour, \"cloud\", None)\n",
    "\n",
    "            t_str     = unwrap_value(t_raw)\n",
    "            temp_val  = unwrap_value(temp_raw)\n",
    "            cloud_val = unwrap_value(cloud_raw)\n",
    "\n",
    "            try:\n",
    "                temp_c = float(temp_val) if temp_val is not None else None\n",
    "            except Exception:\n",
    "                temp_c = None\n",
    "            try:\n",
    "                cloud = int(cloud_val) if cloud_val is not None else None\n",
    "            except Exception:\n",
    "                cloud = None\n",
    "\n",
    "            if t_str:\n",
    "                data.append({\n",
    "                    \"time\": str(t_str),\n",
    "                    \"temp_c\": temp_c,\n",
    "                    \"cloud\": cloud\n",
    "                })\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def extract_weather_from_db(user_object_id, prediction_date):\n",
    "    \"\"\"\n",
    "    1) SELECT current_data FROM weather_data\n",
    "    2) десериализация Java-объекта\n",
    "    3) извлечение forecast → extract_forecast_data\n",
    "    4) DataFrame, to_datetime, ресемплинг 15 минут, интерполяция\n",
    "    Возвращает DataFrame с колонками: time, temp_c, cloud\n",
    "    \"\"\"\n",
    "    sql = text(\"\"\"\n",
    "        SELECT current_data\n",
    "        FROM weather_data\n",
    "        WHERE user_object_id = :uid\n",
    "          AND date = :dt\n",
    "        ORDER BY id\n",
    "        LIMIT 1\n",
    "    \"\"\")\n",
    "\n",
    "    with engine.connect() as conn:\n",
    "        row = conn.execute(sql, {\"uid\": user_object_id, \"dt\": prediction_date}).fetchone()\n",
    "\n",
    "    if not row:\n",
    "        logger.error(\"Няма записи за user_object_id=%s на %s\", user_object_id, prediction_date)\n",
    "        return None\n",
    "\n",
    "    root_obj = deserialize_java_object(row[0])\n",
    "    if root_obj is None:\n",
    "        return None\n",
    "\n",
    "    forecast_obj = getattr(root_obj, \"forecast\", None)\n",
    "    if forecast_obj is None:\n",
    "        logger.error(\"В десериализирания обект няма атрибут forecast\")\n",
    "        return None\n",
    "\n",
    "    raw = extract_forecast_data(forecast_obj)\n",
    "    if not raw:\n",
    "        logger.error(\"extract_forecast_data върна празен списък\")\n",
    "        return None\n",
    "\n",
    "    df = pd.DataFrame(raw)\n",
    "\n",
    "    try:\n",
    "        df[\"time\"] = pd.to_datetime(df[\"time\"], format=\"%Y-%m-%d %H:%M\")\n",
    "    except Exception as e:\n",
    "        logger.error(\"Неуспешна конверсия на време: %s\", e)\n",
    "        return None\n",
    "\n",
    "    df = df.drop_duplicates(subset=\"time\").set_index(\"time\").sort_index()\n",
    "\n",
    "    for col in [\"temp_c\", \"cloud\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    if df[[\"temp_c\", \"cloud\"]].isna().all().all():\n",
    "        logger.error(\"Няма валидни числа в temp_c или cloud за интерполация\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        df15 = df.resample(\"15min\").interpolate(method=\"linear\")\n",
    "    except Exception as e:\n",
    "        logger.error(\"Грешка при ресемплиране: %s\", e)\n",
    "        return None\n",
    "\n",
    "    df15 = df15.reset_index()\n",
    "    df15[\"time\"] = df15[\"time\"].dt.strftime(\"%Y-%m-%d %H:%M\")\n",
    "    return df15\n",
    "\n",
    "# ========== ОСНОВНАЯ ЛОГИКА: ВЫТАЩИТЬ ВСЁ И В ОДИН ФАЙЛ ==========\n",
    "\n",
    "# 1. Берём все доступные даты из weather_data для этого объекта\n",
    "sql_dates = text(\"\"\"\n",
    "    SELECT DISTINCT date\n",
    "    FROM weather_data\n",
    "    WHERE user_object_id = :uid\n",
    "    ORDER BY date\n",
    "\"\"\")\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    df_dates = pd.read_sql(sql_dates, conn, params={\"uid\": user_object_id})\n",
    "\n",
    "available_dates = pd.to_datetime(df_dates[\"date\"]).dt.strftime(\"%Y-%m-%d\").tolist()\n",
    "print(\"Найдено дат с погодой:\", len(available_dates))\n",
    "print(\"Первые 5 дат:\", available_dates[:5])\n",
    "\n",
    "# 2. Итеративно вытаскиваем погоду и складываем в один список\n",
    "all_dfs = []\n",
    "\n",
    "for prediction_date in available_dates:\n",
    "    print(f\"Обрабатываем {prediction_date} ...\")\n",
    "    df_day = extract_weather_from_db(user_object_id, prediction_date)\n",
    "\n",
    "    if df_day is None or df_day.empty:\n",
    "        print(f\"  ⚠ Нет данных (после десериализации) для {prediction_date}\")\n",
    "        continue\n",
    "\n",
    "    df_day[\"date\"] = prediction_date\n",
    "    df_day[\"user_object_id\"] = user_object_id\n",
    "\n",
    "    df_day[\"time\"] = pd.to_datetime(df_day[\"time\"])\n",
    "    all_dfs.append(df_day)\n",
    "\n",
    "# 3. Склеиваем всё и сохраняем в один файл\n",
    "if not all_dfs:\n",
    "    print(\"❌ Не удалось собрать ни одного дня погоды.\")\n",
    "else:\n",
    "    weather_full = pd.concat(all_dfs, ignore_index=True)\n",
    "    weather_full = weather_full.sort_values(\"time\").reset_index(drop=True)\n",
    "\n",
    "    out_csv = os.path.join(OUTPUT_DIR, f\"weather_uid_{user_object_id}_full.csv\")\n",
    "    out_pq  = os.path.join(OUTPUT_DIR, f\"weather_uid_{user_object_id}_full.parquet\")\n",
    "\n",
    "    weather_full.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "    weather_full.to_parquet(out_pq, index=False)\n",
    "\n",
    "    print(\"✅ Готово.\")\n",
    "    print(\"CSV:\", out_csv)\n",
    "    print(\"Parquet:\", out_pq)\n",
    "    display(weather_full.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e6a510-0d5e-4649-9471-d9eca77274dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
